{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecf32e7-c152-4579-94c2-48f8e577375a",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook for Dengue Transfer Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5b9dc-b801-4e0e-8009-3f1f96aa4995",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Conduct EDA for TensorFlow transfer learning pipeline to forecast **weekly dengue cases** (`total_cases`) from 22 multivariate weather/environmental features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88d993-c3b1-4a6c-af85-a4d74a00bb26",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Dengue ML datasets track environmental and temporal factors influencing Aedes mosquito breeding and virus transmission in tropical regions like San Juan and Iquitos.\n",
    "\n",
    "- #### Temporal Features\n",
    "    - **city**: Location identifier (e.g., 'sj' for San Juan, 'iq' for Iquitos)—captures city-specific mosquito/dengue patterns.\n",
    "    - **year, weekofyear, week_start_date**: Time granularity for seasonality; dengue peaks during rainy seasons (weekofyear critical for lagged effects).\n",
    "\n",
    "- #### Vegetation Indices (NDVI)\n",
    "    - **ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw**: Normalized Difference Vegetation Index by city quadrant. Higher NDVI indicates lush vegetation providing mosquito shade/breeding sites; key for Aedes habitat detection via satellite.\n",
    "\n",
    "- #### Precipitation \\& Water\n",
    "    - **precipitation_amt_mm**: Rainfall amount—creates standing water breeding sites.\n",
    "    - **reanalysis_precip_amt_kg_per_m2, reanalysis_sat_precip_amt_mm**: Reanalysis (modeled) precipitation variants confirming observed rain.\n",
    "    - **station_precip_mm**: Ground station measurements—most direct rain proxy.\n",
    "\n",
    "- #### Temperature Metrics\n",
    "    - **reanalysis_air_temp_k, reanalysis_avg_temp_k, reanalysis_max_air_temp_k, reanalysis_min_air_temp_k**: Reanalysis temps in Kelvin; optimal Aedes range 26-32°C accelerates larval development/virus replication.\n",
    "    - **station_avg_temp_c, station_max_temp_c, station_min_temp_c**: Station temps in Celsius—ground truth validation.\n",
    "    - **station_diur_temp_rng_c**: Diurnal range; wider swings stress mosquitoes.\n",
    "    - **reanalysis_tdtr_k**: Temperature diurnal temperature range (reanalysis).\n",
    "\n",
    "- #### Humidity \\& Moisture\n",
    "    - **reanalysis_dew_point_temp_k**: Dew point—direct humidity proxy; high values (>20°C) favor mosquito survival.\n",
    "    - **reanalysis_relative_humidity_percent**: Relative humidity %—critical for egg/larval viability.\n",
    "    - **reanalysis_specific_humidity_g_per_kg**: Absolute moisture content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf887f4-3caa-4743-92fb-64e063d6df4c",
   "metadata": {},
   "source": [
    "### Notebook sections for the second project notebook (Data Cleaning)\n",
    "1. Get Data\n",
    "2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e08f0ea-ff58-4fe2-b0f1-066eea4b4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any, Dict\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "# Set one level up as project root|\n",
    "if os.path.abspath(\"..\") not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "    \n",
    "from src.config import ProjectConfig  # project config file parser\n",
    "from src.utils.eda import value_streaks, top_correlations\n",
    "from src.utils.visualizations import compute_correlations_matrix, \\\n",
    "                display_distributions, random_color, random_colormap, \\\n",
    "                display_timeseries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from src.utils.eda import top_correlations, top_vif\n",
    "from src.utils.utils import _check_feature_presence\n",
    "from src.preprocessing.clean import cap_outliers, drop_nan_rows, \\\n",
    "                                    median_groupwise_impute\n",
    "from src.preprocessing.engineer import reduce_features, remove_features\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "# from matplotlib.axis import Axis\n",
    "# from matplotlib.dates import MonthLocator, YearLocator, DateFormatter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bdf12c-01f6-4f80-87d9-45ac974899f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnfg = ProjectConfig.load_configuration()\n",
    "PATH_TO_RAW_DATA = cnfg.data.dirs[\"raw\"]\n",
    "FILE_TRAIN_RAW= cnfg.data.files[\"features_train\"]\n",
    "FILE_TEST_RAW = cnfg.data.files[\"features_test\"]\n",
    "FILE_LABELS_RAW = cnfg.data.files[\"labels_train\"]\n",
    "TARGET = cnfg.preprocess.feature_groups[\"target\"]\n",
    "ENV_FEAT_PREFIX = cnfg.preprocess.feature_groups[\"env_prefixes\"]\n",
    "CITYGROUP_FEAT = cnfg.preprocess.feature_groups[\"city\"]\n",
    "WEEK_FEAT = cnfg.preprocess.feature_groups[\"week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253393b7-1fff-4956-81d9-63ec1aef02cf",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd2c7d3-1a82-4f2d-a83e-e0ca57449f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TRAIN_RAW, parse_dates=[\"week_start_date\"])\n",
    "df_test_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TEST_RAW, parse_dates=[\"week_start_date\"])\n",
    "df_labels_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_LABELS_RAW)\n",
    "list_raw_df = [df_train_raw, df_test_raw, df_labels_raw]\n",
    "env_features = [f for f in df_train_raw if f.startswith(tuple(ENV_FEAT_PREFIX))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db55d301-c8a9-4207-a36b-704ca034a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TRAIN_RAW, parse_dates=[\"week_start_date\"])\n",
    "# # df_test_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TEST_RAW, parse_dates=[\"week_start_date\"])\n",
    "# df_labels_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_LABELS_RAW)\n",
    "# # list_raw_df = [df_train_raw, df_test_raw, df_labels_raw]\n",
    "# # for df in list_raw_df:\n",
    "# #     display(df.sample(1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344f95e-bc70-4629-b9bb-5dee227b7cc9",
   "metadata": {},
   "source": [
    "***To reduce data snooping, slice last entries for both dataset cities***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8037cfa4-6661-4b33-85c1-67b00f83966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_pct = 0.05\n",
    "cities_first_i = df_train_raw.groupby(by=CITYGROUP_FEAT)[\"week_start_date\"].idxmin()  # Series w Start indices\n",
    "cities_last_i = df_train_raw.groupby(by=CITYGROUP_FEAT)[\"week_start_date\"].idxmax()  # Series w end indices\n",
    "cities_last_i = (cities_last_i - (cities_last_i - cities_first_i) * holdout_pct).astype(int)  # indice math with Series\n",
    "period = tuple(slice(cities_first_i[city], cities_last_i[city], 1) for city in cities_last_i.index[::-1])  # Create tuple of slices from 2 Series\n",
    "df_train_raw_eda = df_train_raw.iloc[np.r_[period]].reset_index(drop=True)  # apply defuned slices\n",
    "df_labels_raw_eda = df_labels_raw.iloc[np.r_[period]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393e6e2-6dfe-4e9c-b4aa-df94046298a1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11f6de-d321-4b11-af46-5a800bad6442",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [x] Cap BEFORE median computation (Winsorization - cap extreme values at specified threshold):\n",
    "    - 1%/99% threshold for both models\n",
    "    - do not cap targets\n",
    "- [X] Remove rows with over 50% of NaN\n",
    "- [X] Impute NaNs with the groupwise median.\n",
    "- [ ] (OPTIONAL, if outliers still there) Reapply outlier handling AFTER imputation with 5%/95% threshold:\n",
    "    - larger threshold preserves more of original distribution shape than tail-focussed 1%/99% threshold\n",
    "    - should not cap much of the data at this stage (CHECK FOR AFFECTED DATAPOINT COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b700b1-baf8-44e0-89f5-d1b4529c1f63",
   "metadata": {},
   "source": [
    "### Outlier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86e3008-9fa9-4c66-8e5f-d35d3cf61dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def cap_outliers(data: pd.DataFrame, features: List[str]=None,\n",
    "#                  group_keys: List[str]=None,\n",
    "#                  lower_cap:float=None, upper_cap:float=None,\n",
    "#                 output_stats:bool=True) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Perform groupwise Winsorization (percentile clipping) on specified features to handle outliers.\n",
    "#     Automatically filter environmental features from config prefixes if not specified.\n",
    "#     :param data: Input pandas DataFrame.\n",
    "#     :param features: List of column names to clip. Default None auto-selects env features \n",
    "#            from prefixes defined in config.yaml.\n",
    "#     :param group_keys: List of columns to group by for quantile calculation. Default None uses \n",
    "#            config.yaml 'city' grouping.\n",
    "#     :param lower_cap: Lower percentile for clipping (0-1). Default None uses config.yaml \n",
    "#            'outlier_perc.lower' (originally 0.01).\n",
    "#     :param upper_cap: Upper percentile for clipping (0-1). Default None uses config.yaml \n",
    "#            'outlier_perc.upper' (originally 0.99).\n",
    "#     :param output_stats: If True, returns % rows changed per feature. Default True.\n",
    "#     :return: Dict containing:\n",
    "#            - 'data': Clipped DataFrame copy (original unchanged)\n",
    "#            - 'capped_row_prc': Series of % rows clipped per feature (if output_stats=True)\n",
    "#     \"\"\"\n",
    "#     if features is None:\n",
    "#         features = [f for f in data.columns if f.startswith(\n",
    "#             tuple(cnfg.preprocess.feature_groups[\"env_prefixes\"]))]\n",
    "#     if group_keys is None:\n",
    "#         group_keys = cnfg.preprocess.feature_groups[\"city\"]\n",
    "#     if lower_cap is None:\n",
    "#         lower_cap = cnfg.preprocess.outlier_perc[\"lower\"]\n",
    "#     if upper_cap is None:\n",
    "#         upper_cap = cnfg.preprocess.outlier_perc[\"upper\"]\n",
    "\n",
    "#     data_no_outliers = data.copy()\n",
    "#     data_no_outliers[features] = data_no_outliers.groupby(by=group_keys)[features].transform(\n",
    "#         lambda group: group.clip(\n",
    "#             lower=group.quantile(lower_cap), upper=group.quantile(upper_cap)))\n",
    "\n",
    "#     if output_stats:\n",
    "#         capped_row_percent = round(\n",
    "#             ((data[features] != data_no_outliers[features]).sum() / len(data) * 100), 2)\n",
    "#         return {\"data\": data_no_outliers,\n",
    "#                 \"capped_row_prc\": capped_row_percent}\n",
    "#     return {\"data\": data_no_outliers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837af435-94c0-40fb-a6d0-be77b78359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = cap_outliers(data=df_train_raw)\n",
    "df_train_clean = intermediate_output[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc17e70-e243-4a51-a8d4-8dba1c5c5034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ndvi_ne                                  15.25\n",
       "ndvi_nw                                   5.63\n",
       "ndvi_se                                   3.71\n",
       "ndvi_sw                                   3.71\n",
       "precipitation_amt_mm                      2.40\n",
       "reanalysis_air_temp_k                     2.82\n",
       "reanalysis_avg_temp_k                     2.82\n",
       "reanalysis_dew_point_temp_k               2.88\n",
       "reanalysis_max_air_temp_k                 2.68\n",
       "reanalysis_min_air_temp_k                 2.61\n",
       "reanalysis_precip_amt_kg_per_m2           2.88\n",
       "reanalysis_relative_humidity_percent      2.88\n",
       "reanalysis_sat_precip_amt_mm              2.40\n",
       "reanalysis_specific_humidity_g_per_kg     2.88\n",
       "reanalysis_tdtr_k                         2.82\n",
       "station_avg_temp_c                        5.01\n",
       "station_diur_temp_rng_c                   5.01\n",
       "station_max_temp_c                        3.09\n",
       "station_min_temp_c                        2.68\n",
       "station_precip_mm                         2.61\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output[\"capped_row_prc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019674b2-b5a6-4cb7-9d58-253b0e34f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment cell for visual check on new distributions for selected features after outlier cliping/Winsorization \n",
    "\n",
    "# selected_distro_EDA_features = [\"ndvi_ne\", \"precipitation_amt_mm\", \"station_diur_temp_rng_c\", \"station_max_temp_c\",\n",
    "#        \"station_min_temp_c\", \"station_precip_mm\"]\n",
    "# # selected_distro_EDA_features = [feature for feature in df_train_raw_eda.select_dtypes(\"float\") if not feature.startswith(\"reanalysis\")]  # used for outlier check\n",
    "\n",
    "# for numeric_feature in selected_distro_EDA_features:\n",
    "#     display_distributions(data=df_train_clean[selected_distro_EDA_features],\n",
    "#                           features=[numeric_feature],\n",
    "#                           title_prefix=numeric_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a1a43-b25f-4fc7-8379-f25914e39eb1",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- Extreme outliers are removed\n",
    "- Data ranges and variations seam to be credible for tropical climate\n",
    "- No need to adjust 1%/99% clipping percentailes or conduct second round of cliping/Winsorization\n",
    "- Adjusted row percentaga is rasonable from ~2-15% (does not exceed 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e32471-0069-4400-813b-343aecb9cbdb",
   "metadata": {},
   "source": [
    "### NaN handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c12bf6-5151-4b0c-86f4-f8b6057fd628",
   "metadata": {},
   "source": [
    "- Remove rows with > 50% NaN values:\n",
    "    - also removes all rowws for `wekofyear` # 53 that do not have observational or analytical data (a likely data collection bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc434c90-b490-48bd-a015-06907a312971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def drop_nan_rows(X: pd.DataFrame, y: pd.Series | None = None,\n",
    "#                   threshold_percent: float = 0.5):\n",
    "#     \"\"\"\n",
    "#     Drop rows with NaN values exceeding threshold_percent of columns.\n",
    "    \n",
    "#     :param X: pandas DataFrame of features.\n",
    "#     :param y: Optional target array/series. Default None.\n",
    "#     :param threshold_percent: Min non-null fraction required [0,1]. Default 0.5.\n",
    "#     :return: Filtered X (and y if provided), both with reset_index().\n",
    "#     \"\"\"\n",
    "#     row_drop_threshold = int(len(X.columns) * threshold_percent)\n",
    "#     result = X.dropna(thresh=row_drop_threshold)\n",
    "#     if y is not None:\n",
    "#         return result.reset_index(), y.iloc[result.index].reset_index()\n",
    "#     return result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa9f582-1dfe-4715-a41e-b378fd7fb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean, df_labels_clean = drop_nan_rows(X=df_train_clean, y=df_labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc9c3b34-34e6-4082-84bb-a95bd6b483d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def median_groupwise_impute(X: pd.DataFrame,\n",
    "#                             group_keys: List[str] = ['city', 'weekofyear']):\n",
    "#     \"\"\"\n",
    "#     Impute NaN values in numeric columns using median within specified group keys.\n",
    "    \n",
    "#     :param X: pandas DataFrame containing grouping columns and features to impute.\n",
    "#     :param group_keys: List of column names for grouping. Default ['city', 'weekofyear'].\n",
    "#     :return: Copy of input DataFrame with NaNs filled by group-wise medians.\n",
    "#     \"\"\"\n",
    "#     missing_keys = set(group_keys) - set(X.columns)\n",
    "#     if missing_keys:\n",
    "#         raise ValueError(f\"Missing group keys {missing_keys}\")\n",
    "\n",
    "#     X_no_nan = X.copy()\n",
    "#     cols_with_nan = X_no_nan.select_dtypes(include=\"number\")\\\n",
    "#         .columns[X_no_nan.select_dtypes(include=\"number\").isna().sum() > 0].to_list()\n",
    "\n",
    "#     if len(cols_with_nan) > 0:\n",
    "#         X_no_nan[cols_with_nan] = X_no_nan[cols_with_nan + group_keys]\\\n",
    "#             .groupby(by=group_keys)[cols_with_nan]\\\n",
    "#             .transform(lambda group: group.fillna(group.median()))\n",
    "#     return X_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276c1f80-f824-4d1a-a7c6-3224c9aca0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean, df_nan_mask = median_groupwise_impute(X=df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd76c48-17ac-48dc-abcd-c3b2a4031960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'year', 'weekofyear', 'total_cases'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329264c2-7b23-4561-999e-99c1a212b936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if any NaNs left\n",
    "df_train_clean.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc39a39-8377-4986-864b-c91531dd65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# plt.figure(figsize=(18, 6))  # df_train_raw.shape[1]\n",
    "# sns.heatmap(\n",
    "#     # median_groupwise_impute(df_train_clean).isna(),\n",
    "#     df_train_clean.isna(),\n",
    "#     cmap='plasma', cbar=False)\n",
    "# plt.title(\"Post-clean NaN location check.\\n\", \n",
    "#           fontsize=13, fontweight=\"bold\")\n",
    "# plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f3029-12cd-4204-8b7b-e7d1d69f8727",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "- primary imputation method: median of all other weekly data from subset of same city data:\n",
    "    - median because data has outliers\n",
    "    - data misigness is acceptable - even in worst cases (after removing week 53 that has no data) thera are more than 50% and at least 6 data pints available to produce `city`+`weekofyear` medians (see EDA table).\n",
    "    - Features in the datset have strong seasonality (rain, temperature, humidity, NDVI). Same week median can handle this.\n",
    "    - moderately well handles different issues with the dataset - scattered NaNs, long streaks (entire season of 15-weeks for `NDVI`)\n",
    "    - simple to implement\n",
    "- Other imputation methods considered:\n",
    "    - Data reconstruction (eg station average or range features from station_max and station_min):\n",
    "        - discarded as performing same calculations on non-nan data show significant discrepancies between calculated and original data\n",
    "    - horizontal imputation from potentially related Reanalysis data:\n",
    "        - discarded: top correlations for station mesurement and reanalysis data do differ accross city data subsets. San Juan has more promissing correlation ranges from ~0.5 (`station_precip_mm`) to ~0.88 (`station_avg_temp_c`) while Iquitos respective ranges are from below 0.4 (`station_precip_mm`) to ~0.6 (`station_avg_temp_c`). Considering that almost all of the missing data for station measurements are in Iquitos, the correlations do not explain enough variance (R^2) and thus median imputation is potentially better tool.\n",
    "    - Temporal interpolation (np.interp, splines):\n",
    "        -  discarded: destroys temporal patterns for long NaN streaks (eg line pattern for entire season or month)\n",
    "    -  KNN/multi-feature models:\n",
    "        - discarded: Complexity vs expected gains. Too much effort and bug risk versus potentially minimal model improvements when simple median imputation used.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a51c3-df65-46e8-be29-98670b55c572",
   "metadata": {},
   "source": [
    "<!-- ### Remove selected multicolinear features\n",
    "- [X] Remove initial `config.yaml` milticolinear features form dataframe\n",
    "- [X] Assess city-wise VIF to EDA instead of correlation matrix for one-vs-all relationships.\n",
    "- [ ] Remove features with VIF > 10\n",
    "- [X] always prefer station_* over reanalysis_* -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5651aea-5ed0-480d-8bcf-5fbb4a3518ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO remove after cleaning\n",
    "\n",
    "# def reduce_features(X: pd.DataFrame, \n",
    "#                     input_feat_groups: List[List[str]]=None,\n",
    "#                     output_feat_names: List[str]=None,\n",
    "#                    function: str=None):\n",
    "#     \"\"\"\n",
    "#     Aggregate multiple feature groups into single reduced features using specified function.\n",
    "#     Combine input features and drop originals.\n",
    "    \n",
    "#     :param X: Input pandas DataFrame.\n",
    "#     :param input_feat_groups: List of feature group lists to aggregate. Default None uses \n",
    "#            config.yaml settings (e.g., [['ndvi_ne', 'ndvi_nw']]).\n",
    "#     :param output_feat_names: Output column names for aggregated features. Default None uses \n",
    "#            config.yaml settings (e.g., ['ndvi_north']).\n",
    "#     :param function: Aggregation function string ('mean', 'sum', 'median'). Default None uses \n",
    "#            config.yaml settings (e.g 'mean').\n",
    "#     :return: DataFrame with reduced features. Original input columns dropped.\n",
    "#     \"\"\"\n",
    "#     X_reduced = X.copy()\n",
    "#     if input_feat_groups is None:\n",
    "#         input_feat_groups = cnfg.preprocess.combine_features[\"input_groups\"]\n",
    "#     if output_feat_names is None:\n",
    "#         output_feat_names = cnfg.preprocess.combine_features[\"output_names\"]\n",
    "#     if function is None:\n",
    "#         function = cnfg.preprocess.combine_features[\"aggregation\"]\n",
    "\n",
    "        \n",
    "#     if not len(input_feat_groups) == len(output_feat_names):\n",
    "#         raise ValueError(f\"Input feature groups {input_feat_groups} mismatch target keys {output_feat_names}\")\n",
    "#     missing_features = set(itertools.chain(*input_feat_groups)) - set(X.columns)\n",
    "#     if missing_features:\n",
    "#         raise ValueError(f\"No {missing_features} features in input dataframe columns: {X.columns}\")\n",
    "\n",
    "#     for name, group in zip(output_feat_names, input_feat_groups):\n",
    "#         X_reduced[name] = X_reduced[group].agg(function, axis=1)\n",
    "#         X_reduced.drop(columns=group, inplace=True)\n",
    "        \n",
    "#     return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5912180-0dab-40f0-b7bb-75f1b35f099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnfg.preprocess.multicolinear[\"removal_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55e9994e-db14-4092-b3c4-0a2a88f1495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO remove after cleaning\n",
    "# def top_vif(data: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Calculate and return Variance Inflation Factor (VIF) scores for numeric features.\n",
    "    \n",
    "#     :param data: pandas DataFrame containing numeric and non-numeric features.\n",
    "#     :return: pandas DataFrame with features and their VIF scores,\n",
    "#                 sorted descending (excludes constant).\n",
    "#     \"\"\"\n",
    "#     data_vif = add_constant(data.select_dtypes(include=\"number\"))\n",
    "#     cols = data_vif.columns\n",
    "#     if data_vif.isna().sum().sum() > 1:\n",
    "#         raise ValueError(f\"{data_vif.isna().sum().sum()} NaNs in the dataframe.\")\n",
    "#     data_vif = [variance_inflation_factor(\n",
    "#         data_vif.values, i) for i in range(data_vif.shape[1])]\n",
    "#     data_vif = pd.DataFrame(data=data_vif, index=cols, columns=[\"vif\"])\n",
    "#     data_vif = data_vif.sort_values(by=\"vif\", ascending=False,\n",
    "#                                     na_position=\"first\").drop(index=\"const\")\n",
    "    \n",
    "#     return data_vif           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5488c88b-7698-4d2f-bf4c-fa2974b62cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_vif_pre = df_train_clean.groupby(by=CITYGROUP_FEAT).apply(lambda group: top_vif(data=group), include_groups=False)\n",
    "# temp_vif_pre =  temp_vif_pre.loc[\"iq\"].join(temp_vif_pre.loc[\"sj\"], lsuffix=\"_iq\", rsuffix=\"_sj\", how=\"inner\")\n",
    "# temp_vif_pre[\"vif_total\"] = top_vif(data=df_train_clean).values\n",
    "# temp_vif_pre.sort_values(by=\"vif_sj\", ascending=False, na_position=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a848c574-4b01-4ddb-8353-3066977952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_clean = remove_features(X=df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c280c65d-3aee-468e-9671-c4c406fbd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_vif_post = df_train_clean.groupby(by=CITYGROUP_FEAT).apply(lambda group: top_vif(data=group), include_groups=False)\n",
    "# temp_vif_post =  temp_vif_post.loc[\"iq\"].join(temp_vif_post.loc[\"sj\"], lsuffix=\"_iq\", rsuffix=\"_sj\", how=\"inner\")\n",
    "# temp_vif_post[\"vif_total\"] = top_vif(data=df_train_clean).values\n",
    "# temp_vif_post.sort_values(by=\"vif_sj\", ascending=False, na_position=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce794c8-1e17-41b1-ab7e-ea716e76a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_threshold=0.8\n",
    "# print(f\"City-stratified correlations exceeding {corr_threshold}:\")\n",
    "# df_train_clean.groupby(\n",
    "#     by=CITYGROUP_FEAT).apply(\n",
    "#         lambda group: top_correlations(\n",
    "#             data=group, \n",
    "#             corr_threshold=corr_threshold\n",
    "#         ), include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73612139-f202-46b0-9930-6f31d20f10d6",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- Overall `vif_total` Variance Inflation Factor cobined for both cities is below 10:\n",
    "  - Among highest VIF scoring overall remains `reanalysis_relative_humidity_percent`, but it is important as there are no alternative station humiidity data and humidity is important for dengue detection domain. \n",
    "- `San Juan` still suffers from higher Multicolinearity that reflects geographical characteristic (more stable climate):\n",
    "    - Top VIF scores for `San Juan` reach 15.2 and are in crucial station  temperature measurements. `station_avg_temp_c` could be potentially removed, but risks hiding important signals in `Iquitos`.\n",
    "    - Keep `station_avg_temp_c` and observe model results:\n",
    "        - VIF score of ~ 15 should be well handled by `LightGBM`\n",
    "        - If `LTSM` `San Juan` -> `Iquitos` transfer learn underperforms, attempt addinf `station_avg_temp_c` to `config.yaml` feature removal list. \n",
    "- VIF factors for `Iquitos` are all acceptable in range below ~6.1.\n",
    "- Remaining high correlation pairs are differenft for both cities confirming that closeness of these features are city specific and thus may encompass important signals for models. So they are kept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e033c0-0e5d-4ecc-ad20-14eaebd13420",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- **Preprocessing tactics for outliers:**\n",
    "    - Target (\"total_cases\")\n",
    "        - If tree models used (eg LightGBM) - no issue, trees are not sensitive to outliers:\n",
    "            - use huber loss for extra safety when handling tails\n",
    "            - RobustScaler may be redunndant for tree models, but if it simplifies pipeline - no harm.\n",
    "        - RNNs (eg LSTM) are outlier sensitive (gradient instability, hidden state patterns loose importance at peaks, scaling):\n",
    "            - Log transform\n",
    "            - Scale (RobustScaler  with IQR is more outlier resistant)\n",
    "            - apply huber loss\n",
    "    - Features:\n",
    "        - RobustScaler for RNN\n",
    "        - RobustScaler may be redunndant for tree models, but if it simplifies pipeline - no harm.\n",
    "- **Preprocessing tactics for low value target streaks**:\n",
    "    - Drop 75 rows of initial zero/low value streaks\n",
    "- Add missigness features that flag NaN rows for top NaN columns that have NaN rate above 1 %:\n",
    "    - Idea - models, especially `LSTM` may learn from informatioun that \"this datapoint was missing\"\n",
    "    - grouped `ndvi_n_missing` and `ndvi_s_missing` (`ndvi_n` and `ndvi_s` will be grouped during feature engneering)\n",
    "    - `station_missing` - group all station data missing flags\n",
    "    - `num_features_imputed` - normalized aggregation of all NaNs vs remaining base features, signals overal uncertainty\n",
    "    - (Optional) During SJ pretraining: Slightly stronger dropout on missingness features than on base features to hide city-correlated missingness\n",
    "    - # TODO: finkcijas loģika (pārnest eng notebook un idzēst):\n",
    "        1. izmet lieko: `[feature for feature in (set(df_nan_mask.columns) - set(df_train_clean.columns)) if not feature.startswith( config.yaml prefixi tuple(\"station\", \"ndvi_s\", \"ndvi_n\"))]`\n",
    "        2. agregē  `num_features_imputed` (ko darīt ar lieajām fīčām, kas ir nomestas pēc NaN imputācijas?)\n",
    "        3. balstoties uz config.yaml prefixiem atlasa fīču grupas un izvelk max, pieloekot \"_missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c8b4f-c886-4390-a098-3933bd7666c3",
   "metadata": {},
   "source": [
    "# TODO: pārorganizēt notebuukus/moduļus:\n",
    "2. DATA CLEANING / PREPROCESSING  ← **log1p belongs here**\n",
    "   - Handle missing values\n",
    "   - Target transformations (log1p, sqrt, Box-Cox) \n",
    "   - Remove duplicates/outliers\n",
    "   - Date parsing\n",
    "3. Feature Engineering              ← iq_initial_low_case_streak belongs here\n",
    "   - Create new features from existing ones\n",
    "   - Interactions, lags, rolling stats\n",
    "   - Domain-specific features\n",
    "4. Feature Selection\n",
    "5. Target processing\n",
    "   - log1p(total_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d87722-c80f-437e-86b1-60635ccd5441",
   "metadata": {},
   "source": [
    "<!-- ### Process zero/low value target value streaks\n",
    "- [ ] Feature engineer flag for early period's low values (ie `iq_initial_low_case_streak`)\n",
    "- [ ] Predict log1p(total_cases) to reduce zero target influence\n",
    "- [ ] (Optional) LSTM specific - if LSTM results are sub-optimal, experiment with downweighting affected period data (ie 0.3 or more) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d852dbc7-0310-43c5-bdb2-2707ec58f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_one_targets = value_streaks(data=df_labels_raw_eda, column=TARGET, value=range(2),\n",
    "#                              run_threshold=5)\n",
    "# print(\"Zero and one consecutive value streaks for target data ('total dengue cases).\")\n",
    "# zero_one_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7cd16-dff0-4dbb-86b5-321d1b21a6f2",
   "metadata": {},
   "source": [
    "### Cleaning pipeline\n",
    "- [ ] run cleaning steps in sequence\n",
    "- [ ] save clean data and nan mask to the disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff91bf9-9df3-4a17-a87e-5ed4210ab962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba: py311_data_prep",
   "language": "python",
   "name": "py311_data_prep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
