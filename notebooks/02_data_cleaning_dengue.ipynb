{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecf32e7-c152-4579-94c2-48f8e577375a",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook for Dengue Transfer Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5b9dc-b801-4e0e-8009-3f1f96aa4995",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Conduct EDA for TensorFlow transfer learning pipeline to forecast **weekly dengue cases** (`total_cases`) from 22 multivariate weather/environmental features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88d993-c3b1-4a6c-af85-a4d74a00bb26",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Dengue ML datasets track environmental and temporal factors influencing Aedes mosquito breeding and virus transmission in tropical regions like San Juan and Iquitos.\n",
    "\n",
    "- #### Temporal Features\n",
    "    - **city**: Location identifier (e.g., 'sj' for San Juan, 'iq' for Iquitos)—captures city-specific mosquito/dengue patterns.\n",
    "    - **year, weekofyear, week_start_date**: Time granularity for seasonality; dengue peaks during rainy seasons (weekofyear critical for lagged effects).\n",
    "\n",
    "- #### Vegetation Indices (NDVI)\n",
    "    - **ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw**: Normalized Difference Vegetation Index by city quadrant. Higher NDVI indicates lush vegetation providing mosquito shade/breeding sites; key for Aedes habitat detection via satellite.\n",
    "\n",
    "- #### Precipitation \\& Water\n",
    "    - **precipitation_amt_mm**: Rainfall amount—creates standing water breeding sites.\n",
    "    - **reanalysis_precip_amt_kg_per_m2, reanalysis_sat_precip_amt_mm**: Reanalysis (modeled) precipitation variants confirming observed rain.\n",
    "    - **station_precip_mm**: Ground station measurements—most direct rain proxy.\n",
    "\n",
    "- #### Temperature Metrics\n",
    "    - **reanalysis_air_temp_k, reanalysis_avg_temp_k, reanalysis_max_air_temp_k, reanalysis_min_air_temp_k**: Reanalysis temps in Kelvin; optimal Aedes range 26-32°C accelerates larval development/virus replication.\n",
    "    - **station_avg_temp_c, station_max_temp_c, station_min_temp_c**: Station temps in Celsius—ground truth validation.\n",
    "    - **station_diur_temp_rng_c**: Diurnal range; wider swings stress mosquitoes.\n",
    "    - **reanalysis_tdtr_k**: Temperature diurnal temperature range (reanalysis).\n",
    "\n",
    "- #### Humidity \\& Moisture\n",
    "    - **reanalysis_dew_point_temp_k**: Dew point—direct humidity proxy; high values (>20°C) favor mosquito survival.\n",
    "    - **reanalysis_relative_humidity_percent**: Relative humidity %—critical for egg/larval viability.\n",
    "    - **reanalysis_specific_humidity_g_per_kg**: Absolute moisture content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf887f4-3caa-4743-92fb-64e063d6df4c",
   "metadata": {},
   "source": [
    "### Notebook sections for the second project notebook (Data Cleaning)\n",
    "1. Get Data\n",
    "2. Data Cleaning\n",
    "4. Feature Selection (TBC poss notebook 03)\n",
    "5. Feature Engineering (TBC poss notebook 03)\n",
    "6. Benchmark Model\n",
    "7. Model Tuning  (TBC)\n",
    "8. Model Evaluation  (TBC, poss notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08f0ea-ff58-4fe2-b0f1-066eea4b4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any, Dict\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "# Set one level up as project root|\n",
    "if os.path.abspath(\"..\") not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "    \n",
    "from src.config import ProjectConfig  # project config file parser\n",
    "from src.utils.eda import value_streaks, top_correlations\n",
    "from src.utils.visualizations import compute_correlations_matrix, \\\n",
    "                display_distributions, random_color, random_colormap, \\\n",
    "                display_timeseries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from src.preprocessing.clean import cap_outliers, drop_nan_rows, \\\n",
    "                                    median_groupwise_impute, reduce_features\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "# from matplotlib.axis import Axis\n",
    "# from matplotlib.dates import MonthLocator, YearLocator, DateFormatter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdf12c-01f6-4f80-87d9-45ac974899f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnfg = ProjectConfig.load_configuration()\n",
    "PATH_TO_RAW_DATA = cnfg.data.dirs[\"raw\"]\n",
    "FILE_TRAIN_RAW= cnfg.data.files[\"features_train\"]\n",
    "FILE_TEST_RAW = cnfg.data.files[\"features_test\"]\n",
    "FILE_LABELS_RAW = cnfg.data.files[\"labels_train\"]\n",
    "TARGET = cnfg.preprocess.feature_groups[\"target\"]\n",
    "ENV_FEAT_PREFIX = cnfg.preprocess.feature_groups[\"env_prefixes\"]\n",
    "CITYGROUP_FEAT = cnfg.preprocess.feature_groups[\"city\"]\n",
    "WEEK_FEAT = cnfg.preprocess.feature_groups[\"week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253393b7-1fff-4956-81d9-63ec1aef02cf",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c7d3-1a82-4f2d-a83e-e0ca57449f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TRAIN_RAW, parse_dates=[\"week_start_date\"])\n",
    "df_test_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TEST_RAW, parse_dates=[\"week_start_date\"])\n",
    "df_labels_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_LABELS_RAW)\n",
    "list_raw_df = [df_train_raw, df_test_raw, df_labels_raw]\n",
    "env_features = [f for f in df_train_raw if f.startswith(tuple(ENV_FEAT_PREFIX))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55d301-c8a9-4207-a36b-704ca034a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TRAIN_RAW, parse_dates=[\"week_start_date\"])\n",
    "# df_test_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_TEST_RAW, parse_dates=[\"week_start_date\"])\n",
    "df_labels_raw = pd.read_csv(PATH_TO_RAW_DATA / FILE_LABELS_RAW)\n",
    "# list_raw_df = [df_train_raw, df_test_raw, df_labels_raw]\n",
    "# for df in list_raw_df:\n",
    "#     display(df.sample(1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344f95e-bc70-4629-b9bb-5dee227b7cc9",
   "metadata": {},
   "source": [
    "***To reduce data snooping, slice last entries for both dataset cities***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037cfa4-6661-4b33-85c1-67b00f83966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_pct = 0.05\n",
    "cities_first_i = df_train_raw.groupby(by=CITYGROUP_FEAT)[\"week_start_date\"].idxmin()  # Series w Start indices\n",
    "cities_last_i = df_train_raw.groupby(by=CITYGROUP_FEAT)[\"week_start_date\"].idxmax()  # Series w end indices\n",
    "cities_last_i = (cities_last_i - (cities_last_i - cities_first_i) * holdout_pct).astype(int)  # indice math with Series\n",
    "period = tuple(slice(cities_first_i[city], cities_last_i[city], 1) for city in cities_last_i.index[::-1])  # Create tuple of slices from 2 Series\n",
    "df_train_raw_eda = df_train_raw.iloc[np.r_[period]].reset_index()  # apply defuned slices\n",
    "df_labels_raw_eda = df_labels_raw.iloc[np.r_[period]].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393e6e2-6dfe-4e9c-b4aa-df94046298a1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11f6de-d321-4b11-af46-5a800bad6442",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [x] Identify outliers per group\n",
    "- [x] Cap BEFORE median computation (Winsorization - cap extreme values at specified threshold):\n",
    "    - 1%/99% threshold for both models \n",
    "- [X] Remove rows with over 50% of NaN\n",
    "- [X] Impute NaNs with the groupwise median.\n",
    "- [ ] combine north and south NDVIs\n",
    "- [ ] (OPTIONAL, if outliers still there) Reapply outlier handling AFTER imputation with 5%/95% threshold:\n",
    "    - larger threshold preserves more of original distribution shape than tail-focussed 1%/99% threshold\n",
    "    - should not cap much of the data at this stage (CHECK FOR AFFECTED DATAPOINT COUNT)\n",
    "- [X] 1%/99% caping threshold for TARGET\n",
    "- [ ] Process zero target value streaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b700b1-baf8-44e0-89f5-d1b4529c1f63",
   "metadata": {},
   "source": [
    "### Outlier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e3008-9fa9-4c66-8e5f-d35d3cf61dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def cap_outliers(data: pd.DataFrame, features: List[str]=None,\n",
    "#                  group_keys: List[str]=None,\n",
    "#                  lower_cap:float=None, upper_cap:float=None,\n",
    "#                 output_stats:bool=True) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Perform groupwise Winsorization (percentile clipping) on specified features to handle outliers.\n",
    "#     Automatically filter environmental features from config prefixes if not specified.\n",
    "#     :param data: Input pandas DataFrame.\n",
    "#     :param features: List of column names to clip. Default None auto-selects env features \n",
    "#            from prefixes defined in config.yaml.\n",
    "#     :param group_keys: List of columns to group by for quantile calculation. Default None uses \n",
    "#            config.yaml 'city' grouping.\n",
    "#     :param lower_cap: Lower percentile for clipping (0-1). Default None uses config.yaml \n",
    "#            'outlier_perc.lower' (originally 0.01).\n",
    "#     :param upper_cap: Upper percentile for clipping (0-1). Default None uses config.yaml \n",
    "#            'outlier_perc.upper' (originally 0.99).\n",
    "#     :param output_stats: If True, returns % rows changed per feature. Default True.\n",
    "#     :return: Dict containing:\n",
    "#            - 'data': Clipped DataFrame copy (original unchanged)\n",
    "#            - 'capped_row_prc': Series of % rows clipped per feature (if output_stats=True)\n",
    "#     \"\"\"\n",
    "#     if features is None:\n",
    "#         features = [f for f in data.columns if f.startswith(\n",
    "#             tuple(cnfg.preprocess.feature_groups[\"env_prefixes\"]))]\n",
    "#     if group_keys is None:\n",
    "#         group_keys = cnfg.preprocess.feature_groups[\"city\"]\n",
    "#     if lower_cap is None:\n",
    "#         lower_cap = cnfg.preprocess.outlier_perc[\"lower\"]\n",
    "#     if upper_cap is None:\n",
    "#         upper_cap = cnfg.preprocess.outlier_perc[\"upper\"]\n",
    "\n",
    "#     data_no_outliers = data.copy()\n",
    "#     data_no_outliers[features] = data_no_outliers.groupby(by=group_keys)[features].transform(\n",
    "#         lambda group: group.clip(\n",
    "#             lower=group.quantile(lower_cap), upper=group.quantile(upper_cap)))\n",
    "\n",
    "#     if output_stats:\n",
    "#         capped_row_percent = round(\n",
    "#             ((data[features] != data_no_outliers[features]).sum() / len(data) * 100), 2)\n",
    "#         return {\"data\": data_no_outliers,\n",
    "#                 \"capped_row_prc\": capped_row_percent}\n",
    "#     return {\"data\": data_no_outliers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837af435-94c0-40fb-a6d0-be77b78359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = cap_outliers(data=df_train_raw)\n",
    "df_train_clean = intermediate_output[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc17e70-e243-4a51-a8d4-8dba1c5c5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[\"capped_row_prc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019674b2-b5a6-4cb7-9d58-253b0e34f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment cell for visual check on new distributions for selected features after outlier cliping/Winsorization \n",
    "\n",
    "# selected_distro_EDA_features = [\"ndvi_ne\", \"precipitation_amt_mm\", \"station_diur_temp_rng_c\", \"station_max_temp_c\",\n",
    "#        \"station_min_temp_c\", \"station_precip_mm\"]\n",
    "# # selected_distro_EDA_features = [feature for feature in df_train_raw_eda.select_dtypes(\"float\") if not feature.startswith(\"reanalysis\")]  # used for outlier check\n",
    "\n",
    "# for numeric_feature in selected_distro_EDA_features:\n",
    "#     display_distributions(data=df_train_clean[selected_distro_EDA_features],\n",
    "#                           features=[numeric_feature],\n",
    "#                           title_prefix=numeric_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a1a43-b25f-4fc7-8379-f25914e39eb1",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- Extreme outliers are removed\n",
    "- Data ranges and variations seam to be credible for tropical climate\n",
    "- No need to adjust 1%/99% clipping percentailes or conduct second round of cliping/Winsorization\n",
    "- Adjusted row percentaga is rasonable from ~2-15% (does not exceed 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150ebd6-5110-42b9-863d-f632ac12953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_target = cap_outliers(data=df_labels_raw, features=TARGET)\n",
    "df_labels_clean = intermediate_target[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9b8a2-018a-4447-8d23-03dd0f5aa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_target[\"capped_row_prc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffafddf-6547-4b76-b790-ccce5eb5b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(data = df_labels_clean,\n",
    "                      hue_palette=(CITYGROUP_FEAT, random_colormap()),\n",
    "                     features=[TARGET], title_prefix=TARGET.capitalize(),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a2af1-aa70-4e6c-a78b-8c0fba850f33",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- Outliers are removed (max 461 in sj -> max of 319)\n",
    "- No need to adjust 1%/99% clipping percentailes or conduct second round of cliping/Winsorization\n",
    "- Adjusted row percentaga is low - 1.37%. Potentially acceptable cost for increased model stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e32471-0069-4400-813b-343aecb9cbdb",
   "metadata": {},
   "source": [
    "### NaN handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c12bf6-5151-4b0c-86f4-f8b6057fd628",
   "metadata": {},
   "source": [
    "- Remove rows with > 50% NaN values:\n",
    "    - also removes all rowws for `wekofyear` # 53 that do not have observational or analytical data (a likely data collection bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc434c90-b490-48bd-a015-06907a312971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def drop_nan_rows(X: pd.DataFrame, y: pd.Series | None = None,\n",
    "#                   threshold_percent: float = 0.5):\n",
    "#     \"\"\"\n",
    "#     Drop rows with NaN values exceeding threshold_percent of columns.\n",
    "    \n",
    "#     :param X: pandas DataFrame of features.\n",
    "#     :param y: Optional target array/series. Default None.\n",
    "#     :param threshold_percent: Min non-null fraction required [0,1]. Default 0.5.\n",
    "#     :return: Filtered X (and y if provided), both with reset_index().\n",
    "#     \"\"\"\n",
    "#     row_drop_threshold = int(len(X.columns) * threshold_percent)\n",
    "#     result = X.dropna(thresh=row_drop_threshold)\n",
    "#     if y is not None:\n",
    "#         return result.reset_index(), y.iloc[result.index].reset_index()\n",
    "#     return result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9f582-1dfe-4715-a41e-b378fd7fb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean, df_labels_clean = drop_nan_rows(X=df_train_clean, y=df_labels_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c3b34-34e6-4082-84bb-a95bd6b483d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# def median_groupwise_impute(X: pd.DataFrame,\n",
    "#                             group_keys: List[str] = ['city', 'weekofyear']):\n",
    "#     \"\"\"\n",
    "#     Impute NaN values in numeric columns using median within specified group keys.\n",
    "    \n",
    "#     :param X: pandas DataFrame containing grouping columns and features to impute.\n",
    "#     :param group_keys: List of column names for grouping. Default ['city', 'weekofyear'].\n",
    "#     :return: Copy of input DataFrame with NaNs filled by group-wise medians.\n",
    "#     \"\"\"\n",
    "#     missing_keys = set(group_keys) - set(X.columns)\n",
    "#     if missing_keys:\n",
    "#         raise ValueError(f\"Missing group keys {missing_keys}\")\n",
    "\n",
    "#     X_no_nan = X.copy()\n",
    "#     cols_with_nan = X_no_nan.select_dtypes(include=\"number\")\\\n",
    "#         .columns[X_no_nan.select_dtypes(include=\"number\").isna().sum() > 0].to_list()\n",
    "\n",
    "#     if len(cols_with_nan) > 0:\n",
    "#         X_no_nan[cols_with_nan] = X_no_nan[cols_with_nan + group_keys]\\\n",
    "#             .groupby(by=group_keys)[cols_with_nan]\\\n",
    "#             .transform(lambda group: group.fillna(group.median()))\n",
    "#     return X_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c1f80-f824-4d1a-a7c6-3224c9aca0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = median_groupwise_impute(X=df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329264c2-7b23-4561-999e-99c1a212b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any NaNs left\n",
    "df_train_clean.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc39a39-8377-4986-864b-c91531dd65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove after cleaning\n",
    "\n",
    "# plt.figure(figsize=(18, 6))  # df_train_raw.shape[1]\n",
    "# sns.heatmap(\n",
    "#     # median_groupwise_impute(df_train_clean).isna(),\n",
    "#     df_train_clean.isna(),\n",
    "#     cmap='plasma', cbar=False)\n",
    "# plt.title(\"Post-clean NaN location check.\\n\", \n",
    "#           fontsize=13, fontweight=\"bold\")\n",
    "# plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07d1d2-7d2a-4ade-807e-9048d4bb1d05",
   "metadata": {},
   "source": [
    "### Combine north and south NDVIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5651aea-5ed0-480d-8bcf-5fbb4a3518ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO remove after cleaning\n",
    "\n",
    "# def reduce_features(X: pd.DataFrame, \n",
    "#                     input_feat_groups: List[List[str]]=None,\n",
    "#                     output_feat_names: List[str]=None,\n",
    "#                    function: str=None):\n",
    "#     \"\"\"\n",
    "#     Aggregate multiple feature groups into single reduced features using specified function.\n",
    "#     Combine input features and drop originals.\n",
    "    \n",
    "#     :param X: Input pandas DataFrame.\n",
    "#     :param input_feat_groups: List of feature group lists to aggregate. Default None uses \n",
    "#            config.yaml settings (e.g., [['ndvi_ne', 'ndvi_nw']]).\n",
    "#     :param output_feat_names: Output column names for aggregated features. Default None uses \n",
    "#            config.yaml settings (e.g., ['ndvi_north']).\n",
    "#     :param function: Aggregation function string ('mean', 'sum', 'median'). Default None uses \n",
    "#            config.yaml settings (e.g 'mean').\n",
    "#     :return: DataFrame with reduced features. Original input columns dropped.\n",
    "#     \"\"\"\n",
    "#     X_reduced = X.copy()\n",
    "#     if input_feat_groups is None:\n",
    "#         input_feat_groups = cnfg.preprocess.combine_features[\"input_groups\"]\n",
    "#     if output_feat_names is None:\n",
    "#         output_feat_names = cnfg.preprocess.combine_features[\"output_names\"]\n",
    "#     if function is None:\n",
    "#         function = cnfg.preprocess.combine_features[\"aggregation\"]\n",
    "\n",
    "        \n",
    "#     if not len(input_feat_groups) == len(output_feat_names):\n",
    "#         raise ValueError(f\"Input feature groups {input_feat_groups} mismatch target keys {output_feat_names}\")\n",
    "#     missing_features = set(itertools.chain(*input_feat_groups)) - set(X.columns)\n",
    "#     if missing_features:\n",
    "#         raise ValueError(f\"No {missing_features} features in input dataframe columns: {X.columns}\")\n",
    "\n",
    "#     for name, group in zip(output_feat_names, input_feat_groups):\n",
    "#         X_reduced[name] = X_reduced[group].agg(function, axis=1)\n",
    "#         X_reduced.drop(columns=group, inplace=True)\n",
    "        \n",
    "#     return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcbf6f-3bdf-4f5c-b9f2-0a907fff4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = reduce_features(df_train_clean)\n",
    "[f for f in df_train_clean.columns if f.startswith(\"ndvi\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b9f07-65aa-47e2-9060-3d3f075b5f3e",
   "metadata": {},
   "source": [
    "- impute NaN values:\n",
    "    - primary imputation method: median of all other weekly data from subset of same city data:\n",
    "        - median because data has outliers\n",
    "        - data misigness is acceptable - even in worst cases (after removing week 53 that has no data) thera are more than 50% and at least 6 data pints available to produce `city`+`weekofyear` medians (see EDA table).\n",
    "        - Features in the datset have strong seasonality (rain, temperature, humidity, NDVI). Same week median can handle this.\n",
    "        - moderately well handles different issues with the dataset - scattered NaNs, long streaks (entire season of 15-weeks for `NDVI`)\n",
    "        - simple to implement\n",
    "        - considers differences in city data subsets discovered during EDA\n",
    "    - Other imputation methods considered:\n",
    "        - Data reconstruction (eg station average or range features from station_max and station_min:\n",
    "            - discarded as performing same calculations on non-nan data show significant discrepancies between calculated and original data\n",
    "        - horizontal imputation from potentially related Reanalysis data:\n",
    "            - discarded: top correlations for station mesurement and reanalysis data do differ accross city data subsets. San Juan has more promissing correlation ranges from ~0.5 (`station_precip_mm`) to ~0.88 (`station_avg_temp_c`) while Iquitos respective ranges are from below 0.4 (`station_precip_mm`) to ~0.6 (`station_avg_temp_c`). Considering that almost all of the missing data for station measurements are in Iquitos, the correlations do not explain enough variance (R^2) and thus median imputation is potentially better tool.\n",
    "        - Temporal interpolation (np.interp, splines):\n",
    "            -  discarded: desroys temporal patterns for long NaN streaks (eg line pattern for entire season or month)\n",
    "        -  KNN/multi-feature models:\n",
    "            - discarded: Complexity vs expected gains. Too much effort and bug risk versus potentially minimal model improvements when simple median imputation used.  \n",
    "    - Add `imputed_featurename` flags to top NaN features that have NaN rate above 1.5 %:\n",
    "        - grouped `ndvi_n` and `ndvi_s` (will be grouped during feature engneering)\n",
    "        - `station_diur_temp_rng_c`\n",
    "        - `station_avg_temp_c`\n",
    "        - `station_precip_mm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e033c0-0e5d-4ecc-ad20-14eaebd13420",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- NaNs:\n",
    "  - potential for row-wise remowal where most of datapoint features lack values\n",
    "  - columnwise *\"ndvi_ne\"* feature:\n",
    "  - sparse, but importand (vegetation data crucial for mosquitoes) - attempt to interpolate from other rows of the same feature\n",
    "  - possible imputation for temperatures where reanalysis data available\n",
    "  - Missing value patterns between San Juan and Iquitos do differ\n",
    "  - Entire season missing for all vegetation data quadrants.:\n",
    "    - keep rows and interpolate data from other years data\n",
    "    - There are different missing data patterns between two datset cities:\n",
    "        - For imputing use only relevant city data as both climate and vegetation differ San Juan and Iquitos\n",
    "  - **Preprocessing tactics for NaNs**:\n",
    "    1) remove obvious rows (eg sparsity > 50%)\n",
    "    2) Figure out trattegy for San Juan that is missing ~20% data for 'ndvi_ne' (most likely keep, if can build other quadrants)\n",
    "    3) check target distribution and if some outlier rows have to be removed\n",
    "    4) reevaluet what is left for imputation:\n",
    "        - imputation options for weather and short term ndvi_ breaks data (time sensitive):\n",
    "            1) Use only relevant city data/group for imputation\n",
    "            2) Impute accordibng to feature importance vs importance for target (mosqiotoe breeding):\n",
    "                1. station_precip_mm (CRITICAL) - Creates breeding sites\n",
    "                2. station_avg_temp_c (CRITICAL) - Optimal 26-32°C for development  \n",
    "                3. station_max_temp_c (HIGH) - Heat stress threshold (>32°C kills)\n",
    "                4. ndvi_ features: Shade/habitat (MEDIUM):\n",
    "                    1) if long missing streeks missing (etire season):\n",
    "                        - 5-12 weeks missing Seasonal mean from same quadrant in all other years for same week\n",
    "                        - 15-18+ weeks\tDROP COLUMN or flag as unreliable - Entire growing season lost \n",
    "                    2) For shorter periods (1-4 weeks) NaNs Use interpolate(method='time') -> lienar between existing time points???)\n",
    "                6. station_min_temp_c (MEDIUM) - Night survival threshold\n",
    "                7. station_diur_temp_rng_c (LOW) - Secondary stress indicator\n",
    "            3) Impute if reanalysis columns available\n",
    "            4) check time sensitive imputation methods (simpler - forward fill last valid value forward with ffill())\n",
    "- multicolinearity:\n",
    "    - Not an issue per se for LSTM, but introduces redundancy. Therefore:\n",
    "        - Remove identical:\n",
    "            -  \"reanalysis_sat_precip_amt_mm\" and near identical \"reanalysis_dew_point_temp_k\"\n",
    "        - remove highly correlated infered feature:\n",
    "            - \"reanalysis_avg_temp_k\"\n",
    "        - keep potential cross domain feature despite high correlations:\n",
    "            - \"reanalysis_tdtr_k\"\n",
    "        - keep direct sation measurement data despite correlations with reanalysis data:\n",
    "            - \"station_diur_temp_rng_c\"\n",
    "        -  cluster vegetation in North and South features:\n",
    "            - 'ndvi_ne' with 'ndvi_nw' and 'ndvi_se' with 'ndvi_sw (AFTER NaN interpolation)\n",
    "- **Preprocessing tactics for outliers:**\n",
    "    - Target (\"total_cases\")\n",
    "        - If tree models used (eg LightGBM) - no issue, trees are not sensitive to outliers:\n",
    "            - use huber loss for extra safety when handling tails\n",
    "            - clip extreme values for dengue context realistic predictions\n",
    "            - RobustScaler may be redunndant for tree models, but if it simplifies pipeline - no harm.\n",
    "        - RNNs (eg LSTM) are outlier sensitive (gradient instability, hidden state patterns loose importance at peaks, scaling):\n",
    "            - Log transform\n",
    "            - Scale (RobustScaler  with IQR is more outlier resistant)\n",
    "            - apply huber loss\n",
    "        - clip (!= remove) extreme values for target for both models, separate clipping by city (outlier in Iquitos may not be an outlier in much larger San Juan)\n",
    "    - Features:\n",
    "        - clip globally (less complex, city specific can mess up transfer learning\n",
    "        - clip for both tree and RNN\n",
    "        - RobustScaler for RNN\n",
    "        - RobustScaler may be redunndant for tree models, but if it simplifies pipeline - no harm.\n",
    "            - rainfall:\n",
    "                - clip to ~ 300 mm (test 99.5 percentile)\n",
    "            - temperature:\n",
    "               - are specific extremes are from 20 - 40 C (test 99.5 percentile)\n",
    "               - clip min to ~ 20\n",
    "               - clip max to ~ 40\n",
    "            - vegetation (ndvi features)\n",
    "                - dataset ranges from -0.456100 (water bodies) to 0.546017 (rainforest) are possible\n",
    "                - no need for clipping, but can apply IQR clip as preventive measure for future data/prediction inputs\n",
    "- **Preprocessing tactics for low value target streaks**:\n",
    "    - Drop 75 rows of initial zero/low value streaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852dbc7-0310-43c5-bdb2-2707ec58f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_targets = value_streaks(data=df_labels_raw_eda, column=TARGET, value=range(2),\n",
    "                             run_threshold=5)\n",
    "print(\"Zero and one consecutive value streaks for target data ('total dengue cases).\")\n",
    "zero_one_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3fab1-62a3-4be2-bce4-da334f6fa7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba: py311_data_prep",
   "language": "python",
   "name": "py311_data_prep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
